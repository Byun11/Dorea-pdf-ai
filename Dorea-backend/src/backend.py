# FastAPI ê´€ë ¨ imports
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

# ë°ì´í„° ëª¨ë¸ ë° ê²€ì¦
from pydantic import BaseModel
from sqlalchemy.orm import Session

# ë‚´ë¶€ ëª¨ë“ˆ
from database import create_database, User, UserSettings, hash_api_key, SessionLocal, PDFFile
from knowledge_routes import router as knowledge_router
from routes.auth_routes import router as auth_router
from routes.folder_routes import router as folder_router
from routes.file_routes import router as file_router
from routes.chat_routes import router as chat_router
from routes.ai_routes import router as ai_router
from routes.model_routes import router as model_router

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import httpx
import os
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime

# Environment variables
DOCKER_API_URL = "http://huridocs:5060"
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://ollama:11434")

# FastAPI ì•± ìƒì„±
app = FastAPI(title="PDF AI ë¶„ì„ ì‹œìŠ¤í…œ")
@app.on_event("startup")
def on_startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸"""
    create_database()
    
    # ì²˜ë¦¬ ì¤‘ ë©ˆì¶˜ íŒŒì¼ ë³µêµ¬
    db = SessionLocal()
    try:
        stuck_files = db.query(PDFFile).filter(PDFFile.status == 'processing').all()
        if stuck_files:
            print(f"âš ï¸ ì„œë²„ ì‹œì‘: {len(stuck_files)}ê°œì˜ ë©ˆì¶˜ íŒŒì¼ì„ 'failed' ìƒíƒœë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
            for file in stuck_files:
                file.status = 'failed'
                file.error_message = "ì„œë²„ê°€ ì²˜ë¦¬ ì¤‘ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
            db.commit()
        else:
            print("âœ… ì„œë²„ ì‹œì‘: ë©ˆì¶°ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‹¤ìŒ ì²˜ë¦¬ ì²´ì¸ ì‹œì‘ ì‹œë„
        from routes.file_routes import trigger_processing_chain
        from fastapi import BackgroundTasks
        background_tasks = BackgroundTasks()
        trigger_processing_chain(db, background_tasks)

    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì¤‘ íŒŒì¼ ìƒíƒœ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
        db.rollback()
    finally:
        db.close()

# ë¼ìš°í„° ë“±ë¡
app.include_router(knowledge_router)
app.include_router(auth_router)
app.include_router(folder_router)
app.include_router(file_router)
app.include_router(chat_router)
app.include_router(ai_router)
app.include_router(model_router)


# Static files ì„¤ì •
STATIC_DIR = Path(__file__).parent / "static"
print(f"âœ… Static directory path: {STATIC_DIR}")
print(f"âœ… Static directory exists: {STATIC_DIR.exists()}")
if STATIC_DIR.exists():
    print(f"âœ… Static directory contents: {list(STATIC_DIR.iterdir())}")
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# íŒŒì¼ ì €ì¥ ê²½ë¡œ
FILES_DIR = Path("/app/DATABASE/files/users")
FILES_DIR.mkdir(parents=True, exist_ok=True)

# Shared Pydantic models
class MultiSegmentRequest(BaseModel):
    """Multi-segment request model used by AI routes"""
    segments: List[Dict[str, Any]]
    query: str
    conversation_history: List[Dict[str, str]] = []  # role, content ìŒì˜ ë¦¬ìŠ¤íŠ¸

# Health check endpoint
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # HURIDOCS API ì—°ê²° í…ŒìŠ¤íŠ¸
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{DOCKER_API_URL}/")
            huridocs_status = "ok" if response.status_code == 200 else "error"
    except:
        huridocs_status = "error"
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "backend": "ok",
            "huridocs": huridocs_status,
            "database": "ok"
        }
    }

# Legacy user AI provider functions (kept for compatibility)
async def get_user_ai_provider(api_key: str, db: Session) -> tuple:
    """ì‚¬ìš©ìì˜ AI ëª¨ë¸ ì„¤ì • ì¡°íšŒ (legacy)"""
    api_key_hash = hash_api_key(api_key)
    
    settings = db.query(UserSettings).filter(
        UserSettings.api_key_hash == api_key_hash
    ).first()
    
    if not settings:
        # ê¸°ë³¸ê°’ ë°˜í™˜ (GPT)
        return "gpt", None
    
    return settings.selected_model_provider, settings.selected_ollama_model

async def get_user_ai_provider_by_user(user: User, db: Session) -> tuple:
    """JWT ì‚¬ìš©ìì˜ AI ëª¨ë¸ ì„¤ì • ì¡°íšŒ - user_id ê¸°ë°˜ìœ¼ë¡œ ì¡°íšŒ"""
    # ğŸ”¥ ì‚¬ìš©ì IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • ì¡°íšŒ (API í‚¤ì™€ ë…ë¦½ì )
    settings = db.query(UserSettings).filter(
        UserSettings.user_id == user.id
    ).first()
    
    if not settings:
        # ê¸°ë³¸ê°’ ë°˜í™˜ (GPT)
        return "gpt", None
    
    return settings.selected_model_provider, settings.selected_ollama_model

async def call_ollama_api(model_name: str, messages: list, stream: bool = False, images: list = None) -> dict:
    """Ollama API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
    try:
        # Ollama API ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        ollama_messages = []
        for msg in messages:
            if msg["role"] == "system":
                ollama_messages.append({"role": "system", "content": msg["content"]})
            elif msg["role"] == "user":
                user_message = {"role": "user", "content": msg["content"]}
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
                if images:
                    user_message["images"] = images
                ollama_messages.append(user_message)
        
        payload = {
            "model": model_name,
            "messages": ollama_messages,
            "stream": stream,
            "options": {
                "keep_alive": "60s"  # ëª¨ë¸ì„ 60ì´ˆ ë™ì•ˆ ë©”ëª¨ë¦¬ì— ìœ ì§€ í›„ ìë™ í•´ì œ
            }
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{OLLAMA_API_URL}/api/chat",
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                if stream:
                    return response  # ìŠ¤íŠ¸ë¦¬ë°ì˜ ê²½ìš° ì‘ë‹µ ê°ì²´ ìì²´ë¥¼ ë°˜í™˜
                else:
                    data = response.json()
                    return {"result": data.get("message", {}).get("content", "")}
            else:
                # APIì—ì„œ ë°›ì€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ì—¬ ì˜ˆì™¸ ë°œìƒ
                error_details = response.text
                raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code} - {error_details}")
                
    except Exception as e:
        raise Exception(f"Ollama ì—°ê²° ì˜¤ë¥˜: {str(e)}")

# íŒŒì¼ ì²˜ë¦¬ ì¢…ë£Œ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
@app.on_event("shutdown")
def cleanup():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    try:
        for file in FILES_DIR.glob("*"):
            if file.is_file():
                file.unlink()
            elif file.is_dir():
                # ë””ë ‰í„°ë¦¬ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì‚¬ìš©ì ë°ì´í„° ë³´í˜¸)
                pass
    except Exception as e:
        print(f"Cleanup ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

# ê°œë°œ ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)