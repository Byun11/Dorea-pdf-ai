# knowledge_manager.py - RAG ì„ë² ë”© ë° ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ

import json
import os
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from pathlib import Path

import chromadb
from chromadb.config import Settings
import openai
import httpx
import httpx
import ollama  # ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ import

# ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ import
from database import SessionLocal, EmbeddingSettings, FileEmbedding, User
from sqlalchemy import or_

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeManager:
    """RAG ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, chroma_path: str = "/app/DATABASE/chroma_db"):
        self.chroma_path = chroma_path
        self.db = SessionLocal()
        self.chroma_client = chromadb.PersistentClient(
            path=chroma_path,
            settings=Settings(anonymized_telemetry=False)
        )
        # Ollama ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        self.ollama_base_url = os.getenv("OLLAMA_API_URL", "http://ollama:11434")
        self.ollama_client = ollama.AsyncClient(host=self.ollama_base_url)
        
        # ë°°ì¹˜ ì§€ì› ìºì‹œ (ëª¨ë¸ë³„ë¡œ í•œ ë²ˆë§Œ í…ŒìŠ¤íŠ¸)
        self.batch_support_cache = {}
        
    async def get_user_settings(self, user_id: int) -> Optional[Dict]:
        """ì‚¬ìš©ì ì„ë² ë”© ì„¤ì • ì¡°íšŒ"""
        settings = self.db.query(EmbeddingSettings).filter_by(user_id=user_id).first()
        if settings:
            return {
                'provider': settings.provider,
                'model_name': settings.model_name,
                'updated_at': settings.updated_at
            }
        return None
    
    async def _get_user_openai_key(self, user_id: int) -> Optional[str]:
        """ì‚¬ìš©ìì˜ OpenAI API í‚¤ ì¡°íšŒ"""
        if not user_id:
            return None
            
        user = self.db.query(User).filter_by(id=user_id).first()
        if user and user.api_key:
            return user.api_key
        return None
    
    async def save_user_settings(self, user_id: int, provider: str, model_name: str) -> bool:
        """ì‚¬ìš©ì ì„ë² ë”© ì„¤ì • ì €ì¥"""
        try:
            settings = self.db.query(EmbeddingSettings).filter_by(user_id=user_id).first()
            
            if settings:
                settings.provider = provider
                settings.model_name = model_name
                settings.updated_at = datetime.utcnow()
            else:
                settings = EmbeddingSettings(
                    user_id=user_id,
                    provider=provider,
                    model_name=model_name
                )
                self.db.add(settings)
            
            self.db.commit()
            logger.info(f"ì‚¬ìš©ì {user_id} ì„ë² ë”© ì„¤ì • ì €ì¥: {provider}/{model_name}")
            return True
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def test_embedding_model(self, provider: str, model_name: str, user_id: int = None) -> Tuple[bool, str]:
        """ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        test_text = "This is a test sentence for embedding."
        
        try:
            if provider == 'ollama':
                return await self._test_ollama_embedding(model_name, test_text)
            elif provider == 'openai':
                return await self._test_openai_embedding(model_name, test_text, user_id)
            else:
                return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}"
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False, str(e)
    
    async def _test_ollama_embedding(self, model_name: str, text: str) -> Tuple[bool, str]:
        """Ollama ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
        try:
            result = await self.ollama_client.embeddings(
                model=model_name,
                prompt=text
            )
            embedding = result.get('embedding')
            if embedding and len(embedding) > 0:
                return True, f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(embedding)}ì°¨ì› ë²¡í„° ìƒì„±"
            else:
                return False, "ì„ë² ë”© ë²¡í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
        except ollama.ResponseError as e:
            logger.error(f"Ollama API ì˜¤ë¥˜: {e.status_code} - {e.error}")
            if e.status_code == 404:
                return False, f"Ollama ëª¨ë¸ '{model_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return False, f"Ollama API ì˜¤ë¥˜: {e.error}"
        except Exception as e:
            logger.error(f"Ollama í…ŒìŠ¤íŠ¸ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return False, f"Ollama í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    async def _test_openai_embedding(self, model_name: str, text: str, user_id: int = None) -> Tuple[bool, str]:
        """OpenAI ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # ì‚¬ìš©ì DBì—ì„œë§Œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key = await self._get_user_openai_key(user_id)
            if not api_key:
                return False, "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            client = openai.AsyncOpenAI(api_key=api_key)
            response = await client.embeddings.create(
                model=model_name,
                input=text
            )
            
            embedding = response.data[0].embedding
            if embedding and len(embedding) > 0:
                return True, f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(embedding)}ì°¨ì› ë²¡í„° ìƒì„±"
            else:
                return False, "ì„ë² ë”© ë²¡í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
                
        except openai.AuthenticationError:
            return False, "OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        except openai.NotFoundError:
            return False, f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        except Exception as e:
            return False, f"OpenAI í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    async def get_file_embedding_status(self, user_id: int, file_id: str) -> Optional[Dict]:
        """íŒŒì¼ì˜ ì„ë² ë”© ìƒíƒœ ì¡°íšŒ"""
        embedding = self.db.query(FileEmbedding).filter_by(
            user_id=user_id, file_id=file_id
        ).first()
        
        if embedding:
            return {
                'file_id': embedding.file_id,
                'filename': embedding.filename,
                'status': embedding.status,
                'total_chunks': embedding.total_chunks,
                'completed_chunks': embedding.completed_chunks,
                'provider': embedding.provider,
                'model_name': embedding.model_name,
                'created_at': embedding.created_at,
                'updated_at': embedding.updated_at,
                'error_message': embedding.error_message,
                'progress': round((embedding.completed_chunks / embedding.total_chunks * 100) 
                                if embedding.total_chunks > 0 else 0, 1)
            }
        return None
    
    async def get_user_embeddings(self, user_id: int) -> List[Dict]:
        """ì‚¬ìš©ìì˜ ëª¨ë“  íŒŒì¼ ì„ë² ë”© ìƒíƒœ ì¡°íšŒ"""
        embeddings = self.db.query(FileEmbedding).filter_by(user_id=user_id).all()
        
        result = []
        for embedding in embeddings:
            progress = round((embedding.completed_chunks / embedding.total_chunks * 100) 
                           if embedding.total_chunks > 0 else 0, 1)
            
            result.append({
                'file_id': embedding.file_id,
                'filename': embedding.filename,
                'status': embedding.status,
                'total_chunks': embedding.total_chunks,
                'completed_chunks': embedding.completed_chunks,
                'provider': embedding.provider,
                'model_name': embedding.model_name,
                'progress': progress,
                'created_at': embedding.created_at,
                'updated_at': embedding.updated_at,
                'error_message': embedding.error_message
            })
        
        return result
    
    def _load_segments_file(self, user_id: int, file_id: str) -> Optional[List[Dict]]:
        """segments.json íŒŒì¼ ë¡œë“œ"""
        base_path = Path("/app/DATABASE/files/users") / str(user_id) / file_id
        segments_files = list(base_path.glob("segments_*.json"))
        if not segments_files:
            logger.error(f"segments íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_path}")
            return None
        
        segments_file = segments_files[0]
        
        try:
            with open(segments_file, 'r', encoding='utf-8') as f:
                segments = json.load(f)
            logger.info(f"segments íŒŒì¼ ë¡œë“œ ì„±ê³µ: {segments_file}, {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            return segments
        except Exception as e:
            logger.error(f"segments íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _filter_valid_segments(self, segments: List[Dict]) -> List[Dict]:
        """ì„ë² ë”©í•  ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§"""
        valid_segments = [s for s in segments if s.get('text', '').strip() and s.get('type') not in ['Page header', 'Page footer']]
        logger.info(f"ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸: {len(valid_segments)}ê°œ (ì „ì²´: {len(segments)}ê°œ)")
        return valid_segments
    
    async def create_file_embedding(self, user_id: int, file_id: str, filename: str) -> bool:
        """íŒŒì¼ì˜ ì„ë² ë”© ìƒì„±"""
        try:
            settings = await self.get_user_settings(user_id)
            if not settings:
                logger.error(f"ì‚¬ìš©ì {user_id}ì˜ ì„ë² ë”© ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            segments = self._load_segments_file(user_id, file_id)
            if not segments: return False
            
            valid_segments = self._filter_valid_segments(segments)
            if not valid_segments: 
                logger.error("ì„ë² ë”©í•  ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            await self._init_file_embedding_status(
                user_id, file_id, filename, len(valid_segments), 
                settings['provider'], settings['model_name']
            )
            
            asyncio.create_task(self._process_embeddings_background(
                user_id, file_id, valid_segments, settings
            ))
            
            return True
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            await self._update_embedding_status(file_id, 'failed', error_message=str(e))
            return False
    
    async def _init_file_embedding_status(self, user_id: int, file_id: str, filename: str, 
                                        total_chunks: int, provider: str, model_name: str):
        """íŒŒì¼ ì„ë² ë”© ìƒíƒœ ì´ˆê¸°í™”"""
        try:
            existing = self.db.query(FileEmbedding).filter_by(user_id=user_id, file_id=file_id).first()
            if existing: self.db.delete(existing)
            
            file_embedding = FileEmbedding(
                file_id=file_id, user_id=user_id, filename=filename,
                status='processing', total_chunks=total_chunks, completed_chunks=0,
                provider=provider, model_name=model_name
            )
            self.db.add(file_embedding)
            self.db.commit()
            logger.info(f"íŒŒì¼ ì„ë² ë”© ìƒíƒœ ì´ˆê¸°í™”: {file_id}, {total_chunks}ê°œ ì²­í¬")
        except Exception as e:
            self.db.rollback()
            logger.error(f"ì„ë² ë”© ìƒíƒœ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _check_cancelled_status(self, file_id: str) -> bool:
        """ì„ë² ë”© ì·¨ì†Œ ìƒíƒœ í™•ì¸"""
        try:
            file_embedding = self.db.query(FileEmbedding).filter_by(file_id=file_id).first()
            if file_embedding and file_embedding.status == 'cancelled':
                logger.info(f"â¹ï¸ ì„ë² ë”© ì·¨ì†Œ ê°ì§€: {file_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"ì·¨ì†Œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    async def _process_embeddings_background(self, user_id: int, file_id: str, 
                                           segments: List[Dict], settings: Dict):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„ë² ë”© ì²˜ë¦¬ (ì·¨ì†Œ ì§€ì›)"""
        try:
            logger.info(f"ì„ë² ë”© ì²˜ë¦¬ ì‹œì‘: {file_id}, {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            provider = settings['provider']
            collection_name = f"user_{user_id}_documents_{provider}"
            collection = self._get_or_create_collection(collection_name)
            await self._delete_file_chunks_from_chroma(collection, file_id)
            model_name = settings['model_name']
            batch_size = 128 if provider == 'openai' else 20  # Ollama ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
            
            # í˜ì´ì§€ë³„ ì¸ë±ìŠ¤ ì¶”ì ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
            page_indices = {}
            
            for i in range(0, len(segments), batch_size):
                # ì·¨ì†Œ ìƒíƒœ ì²´í¬
                if await self._check_cancelled_status(file_id):
                    logger.info(f"â¹ï¸ ì„ë² ë”© ì²˜ë¦¬ ì¤‘ë‹¨ë¨: {file_id}")
                    return
                batch_segments = segments[i:i + batch_size]
                batch_texts = [s['text'] for s in batch_segments]
                
                try:
                    result = await self._generate_batch_embeddings(
                        provider, model_name, batch_texts, user_id
                    )
                    
                    # ì²­í‚¹ ê²°ê³¼ ì²˜ë¦¬
                    if provider == 'ollama' and isinstance(result, tuple) and len(result) == 3:
                        # ì²­í‚¹ëœ ê²°ê³¼: (embeddings, chunk_mapping, chunks)
                        batch_embeddings, chunk_mapping, chunk_texts = result
                        
                        # ì²­í¬ë³„ IDì™€ ë©”íƒ€ë°ì´í„° ìƒì„± (ì›ë³¸ ë³µì‚¬ + ë‚´ìš©ë§Œ ìˆ˜ì •)
                        chunk_ids = []
                        chunk_metadatas = []
                        chunk_index = 0
                        
                        for j, segment in enumerate(batch_segments):
                            # í˜ì´ì§€ë³„ ìƒëŒ€ ì¸ë±ìŠ¤ ê³„ì‚°
                            page_num = int(segment.get('page_number', 1))
                            if page_num not in page_indices:
                                page_indices[page_num] = 0
                            page_relative_index = page_indices[page_num]
                            page_indices[page_num] += 1
                            
                            orig_chunks_count = chunk_mapping.count(j)
                            for k in range(orig_chunks_count):
                                chunk_ids.append(f"{file_id}_{i+j}_{k}")
                                
                                # ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³µì‚¬ í›„ ì²­í¬ë³„ ìˆ˜ì •
                                metadata = {
                                    'file_id': file_id, 
                                    'user_id': str(user_id),
                                    'chunk_index': i+j, 
                                    'sub_chunk': k,
                                    'segment_id': segment.get('id') or f"page{page_num}_{page_relative_index}",
                                    'segment_type': segment.get('type', 'text'),
                                    'page_number': page_num,
                                    'text_length': len(chunk_texts[chunk_index + k])
                                }
                                # ì›ë³¸ segmentì˜ ë‹¤ë¥¸ í•„ë“œë“¤ë„ ë³µì‚¬
                                for key, value in segment.items():
                                    if key not in ['text'] and key not in metadata:
                                        metadata[key] = value
                                        
                                chunk_metadatas.append(metadata)
                            chunk_index += orig_chunks_count
                        
                        logger.info(f"ğŸ” ì²­í‚¹ ì„ë² ë”© ê²°ê³¼: {len(batch_embeddings)}ê°œ ì²­í¬ (ì›ë³¸ {len(batch_texts)}ê°œ)")
                        
                        # ChromaDBì— ì²­í¬ë³„ë¡œ ì €ì¥
                        try:
                            collection.add(embeddings=batch_embeddings, documents=chunk_texts, ids=chunk_ids, metadatas=chunk_metadatas)
                            await self._update_embedding_progress(file_id, i + len(batch_segments))
                            logger.info(f"ë°°ì¹˜ {i+1}-{i+len(batch_segments)}/{len(segments)} ì²­í‚¹ ì„ë² ë”© ì™„ë£Œ: {file_id}")
                        except Exception as chroma_error:
                            logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨: {chroma_error}")
                            if "dimension" in str(chroma_error).lower():
                                logger.warning(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ ì»¬ë ‰ì…˜ ì¬ìƒì„±: {collection_name}")
                                self.chroma_client.delete_collection(collection_name)
                                collection = self._get_or_create_collection(collection_name)
                                collection.add(embeddings=batch_embeddings, documents=chunk_texts, ids=chunk_ids, metadatas=chunk_metadatas)
                            else:
                                raise chroma_error
                            await self._update_embedding_progress(file_id, i + len(batch_segments))
                            logger.info(f"ë°°ì¹˜ {i+1}-{i+len(batch_segments)}/{len(segments)} ì²­í‚¹ ì„ë² ë”© ì™„ë£Œ (ì»¬ë ‰ì…˜ ì¬ìƒì„±): {file_id}")
                            
                    else:
                        # ì¼ë°˜ ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬
                        batch_embeddings = result
                        logger.info(f"ğŸ” ë°°ì¹˜ ì„ë² ë”© ê²°ê³¼: {len(batch_embeddings) if batch_embeddings else 'None'}, ì˜ˆìƒ: {len(batch_texts)}")
                        
                        if not batch_embeddings or len(batch_embeddings) != len(batch_texts):
                            logger.error(f"âŒ ë°°ì¹˜ {i}-{i+batch_size} ì„ë² ë”© ê²°ê³¼ ê¸¸ì´ ë¶ˆì¼ì¹˜")
                            continue

                        chunk_ids = [f"{file_id}_{i+j}" for j in range(len(batch_segments))]
                        metadatas = []
                        for j, s in enumerate(batch_segments):
                            # í˜ì´ì§€ë³„ ìƒëŒ€ ì¸ë±ìŠ¤ ê³„ì‚°
                            page_num = int(s.get('page_number', 1))
                            if page_num not in page_indices:
                                page_indices[page_num] = 0
                            page_relative_index = page_indices[page_num]
                            page_indices[page_num] += 1
                            
                            metadatas.append({
                                'file_id': file_id, 'user_id': str(user_id),
                                'chunk_index': i+j, 
                                'segment_id': s.get('id') or f"page{page_num}_{page_relative_index}",
                                'segment_type': s.get('type', 'text'),
                                'page_number': page_num,
                                'text_length': len(s['text'])
                            })

                        try:
                            collection.add(embeddings=batch_embeddings, documents=batch_texts, ids=chunk_ids, metadatas=metadatas)
                            await self._update_embedding_progress(file_id, i + len(batch_segments))
                            logger.info(f"ë°°ì¹˜ {i+1}-{i+len(batch_segments)}/{len(segments)} ì„ë² ë”© ì™„ë£Œ: {file_id}")
                        except Exception as chroma_error:
                            logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨: {chroma_error}")
                            if "dimension" in str(chroma_error).lower():
                                logger.warning(f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ ì»¬ë ‰ì…˜ ì¬ìƒì„±: {collection_name}")
                                self.chroma_client.delete_collection(collection_name)
                                collection = self._get_or_create_collection(collection_name)
                                collection.add(embeddings=batch_embeddings, documents=batch_texts, ids=chunk_ids, metadatas=metadatas)
                            else:
                                raise chroma_error
                            await self._update_embedding_progress(file_id, i + len(batch_segments))
                            logger.info(f"ë°°ì¹˜ {i+1}-{i+len(batch_segments)}/{len(segments)} ì„ë² ë”© ì™„ë£Œ (ì»¬ë ‰ì…˜ ì¬ìƒì„±): {file_id}")

                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {i}-{i+batch_size} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                
                await asyncio.sleep(0.05)
            
            await self._update_embedding_status(file_id, 'completed')
            logger.info(f"íŒŒì¼ ì„ë² ë”© ì™„ë£Œ: {file_id}")
        except Exception as e:
            logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            await self._update_embedding_status(file_id, 'failed', error_message=str(e))
    
    async def _update_embedding_progress(self, file_id: str, completed_chunks: int):
        """ì„ë² ë”© ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            file_embedding = self.db.query(FileEmbedding).filter_by(file_id=file_id).first()
            if file_embedding:
                file_embedding.completed_chunks = completed_chunks
                file_embedding.updated_at = datetime.utcnow()
                self.db.commit()
                
                # ì§„í–‰ë¥  ê³„ì‚° ë° ë¡œê·¸
                progress = round((completed_chunks / file_embedding.total_chunks * 100), 1)
                logger.info(f"ğŸ“Š DB ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {file_id} â†’ {completed_chunks}/{file_embedding.total_chunks} ({progress}%)")
        except Exception as e:
            self.db.rollback()
            logger.error(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def _update_embedding_status(self, file_id: str, status: str, error_message: str = None):
        """ì„ë² ë”© ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            file_embedding = self.db.query(FileEmbedding).filter_by(file_id=file_id).first()
            if file_embedding:
                file_embedding.status = status
                file_embedding.updated_at = datetime.utcnow()
                if error_message: file_embedding.error_message = error_message
                if status == 'completed':
                    file_embedding.completed_chunks = file_embedding.total_chunks
                self.db.commit()
                logger.info(f"ìƒíƒœ ì—…ë°ì´íŠ¸: {file_id} -> {status}")
        except Exception as e:
            self.db.rollback()
            logger.error(f"ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _get_or_create_collection(self, collection_name: str):
        """ChromaDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        try:
            return self.chroma_client.get_collection(collection_name)
        except ValueError:
            return self.chroma_client.create_collection(name=collection_name)
    
    async def _generate_batch_embeddings(self, provider: str, model_name: str, 
                                       texts: List[str], user_id: int) -> Optional[List[List[float]]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            if provider == 'ollama':
                return await self._generate_ollama_batch_embeddings(model_name, texts)
            elif provider == 'openai':
                return await self._generate_openai_batch_embeddings(model_name, texts, user_id)
            return None
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    async def _generate_embedding(self, provider: str, model_name: str, 
                                text: str, user_id: int) -> Optional[List[float]]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        batch_result = await self._generate_batch_embeddings(provider, model_name, [text], user_id)
        
        # Ollama ì²­í‚¹ ê²°ê³¼ ì²˜ë¦¬
        if isinstance(batch_result, tuple) and len(batch_result) == 3:
            embeddings, chunk_mapping, chunks = batch_result
            # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ì²­í¬ ì„ë² ë”© ë°˜í™˜
            return embeddings[0] if embeddings else None
        
        # ì¼ë°˜ ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬
        return batch_result[0] if batch_result else None
    
    async def _generate_openai_batch_embeddings(self, model_name: str, texts: List[str], user_id: int) -> Optional[List[List[float]]]:
        """OpenAIë¡œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        try:
            api_key = await self._get_user_openai_key(user_id)
            if not api_key: raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            client = openai.AsyncOpenAI(api_key=api_key)
            response = await client.embeddings.create(model=model_name, input=texts)
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"OpenAI ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def _get_ollama_model_context_length(self, model_name: str) -> int:
        """Ollama ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸"""
        try:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://ollama:11434/api/show",
                    json={"name": model_name}
                )
                if response.status_code == 200:
                    model_info = response.json()
                    # model_infoì—ì„œ context_length ì°¾ê¸°
                    if "model_info" in model_info:
                        context_length = model_info["model_info"].get("bert.context_length")
                        if context_length:
                            return int(context_length)
                    return 512  # ê¸°ë³¸ê°’
                return 512
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ 512 ì‚¬ìš©: {e}")
            return 512

    def _chunk_text(self, text: str, max_chars: int) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹ ì²˜ë¦¬ (ê²¹ì¹¨ í¬í•¨)"""
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        overlap = int(max_chars * 0.1)  # 10% ê²¹ì¹¨
        start = 0
        
        while start < len(text):
            end = start + max_chars
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            chunk = text[start:end]
            last_space = chunk.rfind(' ')
            if last_space > max_chars * 0.8:  # 80% ì´ìƒ ìœ„ì¹˜ì—ì„œ ê³µë°± ë°œê²¬
                end = start + last_space
            
            chunks.append(text[start:end])
            start = end - overlap
        
        return chunks

    async def _generate_ollama_batch_embeddings(self, model_name: str, texts: List[str]) -> Optional[List[List[float]]]:
        """Ollama ì„ë² ë”© ìƒì„± (ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹ í¬í•¨)"""
        try:
            # ëª¨ë¸ì˜ ì‹¤ì œ í† í° ì œí•œ í™•ì¸
            max_tokens = await self._get_ollama_model_context_length(model_name)
            max_chars = int(max_tokens * 0.8)
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹ ì²˜ë¦¬
            all_chunks = []
            chunk_mapping = []  # ì›ë³¸ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ë§¤í•‘
            
            for i, text in enumerate(texts):
                chunks = self._chunk_text(text, max_chars)
                all_chunks.extend(chunks)
                chunk_mapping.extend([i] * len(chunks))
                
                if len(chunks) > 1:
                    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ {i+1} ì²­í‚¹: {len(text)}ì â†’ {len(chunks)}ê°œ ì²­í¬")
            
            logger.info(f"ğŸ”„ Ollama ë°°ì¹˜ ì²˜ë¦¬: {len(texts)}ê°œ í…ìŠ¤íŠ¸ â†’ {len(all_chunks)}ê°œ ì²­í¬")
            
            # ì²­í¬ë“¤ì„ ë°°ì¹˜ ì²˜ë¦¬
            chunk_embeddings = []
            batch_size = 20  # ì²­í¬ ë°°ì¹˜ í¬ê¸°
            
            for i in range(0, len(all_chunks), batch_size):
                batch_chunks = all_chunks[i:i + batch_size]
                
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.post(
                            f"{self.ollama_base_url}/api/embed",
                            json={
                                "model": model_name,
                                "input": batch_chunks
                            },
                            timeout=60.0
                        )
                        response.raise_for_status()
                        response_data = response.json()
                    
                    embeddings = response_data.get("embeddings", [])
                    if embeddings and len(embeddings) == len(batch_chunks):
                        chunk_embeddings.extend(embeddings)
                        logger.info(f"âœ… ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ: {len(embeddings)}ê°œ")
                    else:
                        logger.warning(f"âŒ ì²­í¬ ë°°ì¹˜ ì‘ë‹µ ê¸¸ì´ ë¶ˆì¼ì¹˜")
                        return None
                        
                except Exception as batch_error:
                    logger.warning(f"âŒ ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {batch_error}")
                    return None
            
            # ê° ì²­í¬ë¥¼ ë…ë¦½ì ì¸ ì„ë² ë”©ìœ¼ë¡œ ë°˜í™˜ (ì˜ë¯¸ ë³´ì¡´)
            logger.info(f"âœ… ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ: {len(chunk_embeddings)}ê°œ ì²­í¬ ì„ë² ë”© (ì›ë³¸ {len(texts)}ê°œ í…ìŠ¤íŠ¸)")
            return chunk_embeddings, chunk_mapping, all_chunks  # ì²­í¬ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜

        except Exception as e:
            logger.error(f"Ollama ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return await self._generate_ollama_individual_embeddings(model_name, texts)
    
    async def _generate_ollama_individual_embeddings(self, model_name: str, texts: List[str]) -> Optional[List[List[float]]]:
        """Ollama ê°œë³„ ì²˜ë¦¬ (í´ë°±ìš©)"""
        try:
            max_tokens = await self._get_ollama_model_context_length(model_name)
            max_chars = int(max_tokens * 0.8)
            
            embeddings = []
            total_texts = len(texts)
            
            for i, text in enumerate(texts):
                truncated_text = text[:max_chars] if len(text) > max_chars else text
                
                try:
                    response = await self.ollama_client.embeddings(
                        model=model_name,
                        prompt=truncated_text
                    )
                    embeddings.append(response["embedding"])
                    logger.info(f"Ollama ê°œë³„ ì„ë² ë”© ìƒì„±ë¨ (ì²« 5ê°œ ê°’): {response['embedding'][:5]}...")
                    
                    # ì§„í–‰ë¥  ë¡œê·¸
                    if (i + 1) % 5 == 0 or i == total_texts - 1:
                        progress = round((i + 1) / total_texts * 100, 1)
                        logger.info(f"Ollama ê°œë³„ ì„ë² ë”© ì§„í–‰: {i + 1}/{total_texts} ({progress}%)")
                    
                except Exception as single_error:
                    logger.error(f"ê°œë³„ ì„ë² ë”© {i+1}/{total_texts} ì‹¤íŒ¨: {single_error}")
                    # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•˜ì§€ ì•Šê³  ì „ì²´ ì‹¤íŒ¨ ì²˜ë¦¬
                    return None
                
                # ê°œë³„ ì²˜ë¦¬ ì‹œ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(0.1)
                    
            return embeddings
        except Exception as e:
            logger.error(f"Ollama ê°œë³„ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def cancel_file_embedding(self, user_id: int, file_id: str) -> bool:
        """íŒŒì¼ì˜ ì„ë² ë”© ì²˜ë¦¬ ì·¨ì†Œ"""
        try:
            file_embedding = self.db.query(FileEmbedding).filter_by(user_id=user_id, file_id=file_id).first()
            if not file_embedding or file_embedding.status != 'processing':
                return False
            
            file_embedding.status = 'cancelled'
            file_embedding.updated_at = datetime.utcnow()
            file_embedding.error_message = 'ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨'
            self.db.commit()
            
            # íŒŒì¼ì˜ ì„ë² ë”© í”„ë¡œë°”ì´ë” ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            embedding_provider = file_embedding.provider if file_embedding.provider else 'ollama'
            collection_name = f"user_{user_id}_documents_{embedding_provider}"
            try:
                collection = self.chroma_client.get_collection(collection_name)
                await self._delete_file_chunks_from_chroma(collection, file_id)
            except ValueError:
                logger.warning(f"ChromaDB ì»¬ë ‰ì…˜ {collection_name} ì •ë¦¬ ì¤‘ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë¬´ì‹œ)")
            
            logger.info(f"ì„ë² ë”© ì·¨ì†Œ ì™„ë£Œ: {file_id}")
            return True
        except Exception as e:
            self.db.rollback()
            logger.error(f"ì„ë² ë”© ì·¨ì†Œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _delete_file_chunks_from_chroma(self, collection, file_id: str):
        """ChromaDBì—ì„œ íŒŒì¼ì˜ ê¸°ì¡´ ì²­í¬ë“¤ ì‚­ì œ"""
        results = collection.get(where={"file_id": file_id}, include=[])
        if results['ids']:
            collection.delete(ids=results['ids'])
            logger.info(f"ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ: {file_id}, {len(results['ids'])}ê°œ")
    
    async def _check_embedding_consistency(self, user_id: int, current_settings: Dict, file_id: str = None) -> List[Dict]:
        """ì„ë² ë”© ëª¨ë¸ í†µì¼ì„± ì²´í¬"""
        try:
            # íŠ¹ì • íŒŒì¼ë§Œ ì²´í¬í•˜ëŠ” ê²½ìš°
            if file_id:
                embedding = self.db.query(FileEmbedding).filter_by(
                    user_id=user_id, file_id=file_id, status='completed'
                ).first()
                
                if embedding and (embedding.provider != current_settings['provider'] or 
                                embedding.model_name != current_settings['model_name']):
                    return [{
                        'file_id': embedding.file_id,
                        'filename': embedding.filename,
                        'existing_model': f"{embedding.provider}:{embedding.model_name}",
                        'current_model': f"{current_settings['provider']}:{current_settings['model_name']}"
                    }]
                return []
            
            # ì „ì²´ íŒŒì¼ ì²´í¬
            inconsistent_embeddings = self.db.query(FileEmbedding).filter(
                FileEmbedding.user_id == user_id,
                FileEmbedding.status == 'completed',
                or_(
                    FileEmbedding.provider != current_settings['provider'],
                    FileEmbedding.model_name != current_settings['model_name']
                )
            ).all()
            
            result = []
            for embedding in inconsistent_embeddings:
                result.append({
                    'file_id': embedding.file_id,
                    'filename': embedding.filename,
                    'existing_model': f"{embedding.provider}:{embedding.model_name}",
                    'current_model': f"{current_settings['provider']}:{current_settings['model_name']}"
                })
            
            return result
        except Exception as e:
            logger.error(f"ì„ë² ë”© í†µì¼ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
            return []
    
    async def reembed_inconsistent_files(self, user_id: int) -> Dict[str, Any]:
        """ëª¨ë¸ ë¶ˆì¼ì¹˜ íŒŒì¼ë“¤ ì¬ì„ë² ë”©"""
        try:
            settings = await self.get_user_settings(user_id)
            if not settings:
                return {"success": False, "message": "ì„ë² ë”© ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"}
            
            inconsistent_files = await self._check_embedding_consistency(user_id, settings)
            if not inconsistent_files:
                return {"success": True, "message": "ì¬ì„ë² ë”©ì´ í•„ìš”í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤", "count": 0}
            
            success_count = 0
            failed_files = []
            
            for file_info in inconsistent_files:
                file_id = file_info['file_id']
                filename = file_info['filename']
                
                try:
                    # ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ í›„ ì¬ìƒì„±
                    await self.delete_file_embedding(user_id, file_id)
                    success = await self.create_file_embedding(user_id, file_id, filename)
                    
                    if success:
                        success_count += 1
                        logger.info(f"âœ… ì¬ì„ë² ë”© ì‹œì‘: {filename}")
                    else:
                        failed_files.append(filename)
                        logger.error(f"âŒ ì¬ì„ë² ë”© ì‹¤íŒ¨: {filename}")
                        
                except Exception as e:
                    failed_files.append(filename)
                    logger.error(f"âŒ ì¬ì„ë² ë”© ì˜¤ë¥˜: {filename} - {e}")
            
            message = f"{success_count}ê°œ íŒŒì¼ì˜ ì¬ì„ë² ë”©ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."
            if failed_files:
                message += f" (ì‹¤íŒ¨: {len(failed_files)}ê°œ)"
            
            return {
                "success": True, 
                "message": message,
                "count": success_count,
                "failed_files": failed_files
            }
            
        except Exception as e:
            logger.error(f"ì¼ê´„ ì¬ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return {"success": False, "message": f"ì¬ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    async def delete_file_embedding(self, user_id: int, file_id: str) -> bool:
        """íŒŒì¼ì˜ ì„ë² ë”© ì‚­ì œ"""
        try:
            # íŒŒì¼ì˜ ì„ë² ë”© í”„ë¡œë°”ì´ë” ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            file_embedding = self.db.query(FileEmbedding).filter_by(user_id=user_id, file_id=file_id).first()
            embedding_provider = file_embedding.provider if file_embedding and file_embedding.provider else 'ollama'
            collection_name = f"user_{user_id}_documents_{embedding_provider}"
            try:
                collection = self.chroma_client.get_collection(collection_name)
                await self._delete_file_chunks_from_chroma(collection, file_id)
            except ValueError:
                logger.warning(f"ChromaDB ì»¬ë ‰ì…˜ {collection_name} ì‚­ì œ ì¤‘ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë¬´ì‹œ)")
            
            self.db.query(FileEmbedding).filter_by(user_id=user_id, file_id=file_id).delete()
            self.db.commit()
            logger.info(f"íŒŒì¼ ì„ë² ë”© ì‚­ì œ ì™„ë£Œ: {file_id}")
            return True
        except Exception as e:
            self.db.rollback()
            logger.error(f"íŒŒì¼ ì„ë² ë”© ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    async def search_similar_documents(self, user_id: int, query: str, 
                                     top_k: int = 5, file_id: str = None) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì„ íƒì ìœ¼ë¡œ íŠ¹ì • íŒŒì¼ë¡œ ì œí•œ)"""
        try:
            embedding_provider = None
            embedding_model = None

            if file_id:
                # íŠ¹ì • íŒŒì¼ ê²€ìƒ‰ ì‹œ, í•´ë‹¹ íŒŒì¼ì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
                file_embedding_info = await self.get_file_embedding_status(user_id, file_id)
                if file_embedding_info and file_embedding_info['status'] == 'completed':
                    embedding_provider = file_embedding_info['provider']
                    embedding_model = file_embedding_info['model_name']
                    logger.info(f"ğŸ“„ íŒŒì¼ë³„ ê²€ìƒ‰: '{file_embedding_info['filename']}'ì˜ ì„ë² ë”© ëª¨ë¸({embedding_provider}/{embedding_model})ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                else:
                    logger.warning(f"âš ï¸ í•´ë‹¹ íŒŒì¼({file_id})ì˜ ì„ë² ë”© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return []
            
            if not embedding_provider or not embedding_model:
                # ì „ì—­ ì„¤ì • ì‚¬ìš© (íŒŒì¼ IDê°€ ì—†ê±°ë‚˜, íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš°)
                settings = await self.get_user_settings(user_id)
                if not settings: 
                    logger.error(f"âŒ ì‚¬ìš©ì {user_id}ì˜ ì„ë² ë”© ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ì„ ìœ„í•´ì„œëŠ” ë¨¼ì € ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                    return []
                embedding_provider = settings['provider']
                embedding_model = settings['model_name']
                logger.info(f"âš™ï¸ ì „ì—­ ê²€ìƒ‰: í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ëª¨ë¸({embedding_provider}/{embedding_model})ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                # ì „ì—­ ê²€ìƒ‰ ì‹œì—ë§Œ ëª¨ë¸ ë¶ˆì¼ì¹˜ ê²€ì‚¬ ìˆ˜í–‰
                inconsistent_files = await self._check_embedding_consistency(user_id, settings)
                if inconsistent_files:
                    logger.warning(f"âš ï¸ ì „ì—­ ì„¤ì •ê³¼ ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” íŒŒì¼ì´ {len(inconsistent_files)}ê°œ ìˆìŠµë‹ˆë‹¤.")
                    return [{
                        "type": "embedding_inconsistency_warning", 
                        "inconsistent_files": inconsistent_files,
                        "current_model": f"{settings['provider']}:{settings['model_name']}",
                        "message": f"í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ëª¨ë¸ê³¼ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì„ë² ë”©ëœ íŒŒì¼ë“¤ì´ ìˆìŠµë‹ˆë‹¤."
                    }]

            query_embedding = await self._generate_embedding(
                embedding_provider, embedding_model, query, user_id
            )
            if not query_embedding: 
                logger.error(f"âŒ ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: query='{query}', provider={embedding_provider}, model={embedding_model}")
                return []
            
            if not isinstance(query_embedding, list) or not query_embedding:
                logger.error(f"âŒ ì˜ëª»ëœ ì„ë² ë”© í˜•ì‹: {type(query_embedding)}")
                return []
            
            logger.info(f"âœ… ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì„±ê³µ: ì°¨ì›={len(query_embedding)}")
            
            collection_name = f"user_{user_id}_documents_{embedding_provider}"
            try:
                collection = self.chroma_client.get_collection(collection_name)
                
                # ì»¬ë ‰ì…˜ ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸
                total_count = collection.count()
                logger.info(f"ğŸ“Š ì»¬ë ‰ì…˜ '{collection_name}' ì „ì²´ ë¬¸ì„œ ê°œìˆ˜: {total_count}")
                
                # í•´ë‹¹ íŒŒì¼ì˜ ë°ì´í„° ê°œìˆ˜ í™•ì¸
                if file_id:
                    file_results = collection.get(where={"file_id": file_id}, include=[])
                    file_count = len(file_results['ids']) if file_results['ids'] else 0
                    logger.info(f"ğŸ“„ íŒŒì¼ '{file_id}'ì˜ ì„ë² ë”© ê°œìˆ˜: {file_count}")
                    
                    if file_count == 0:
                        logger.warning(f"âš ï¸ íŒŒì¼ '{file_id}'ì˜ ì„ë² ë”©ì´ ChromaDBì— ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”© ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                        
            except ValueError: 
                logger.error(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            where_filter = {"file_id": file_id} if file_id else None
            logger.info(f"ğŸ” ChromaDB ê²€ìƒ‰ ì¡°ê±´: collection={collection_name}, filter={where_filter}")
            
            results = collection.query(
                query_embeddings=[query_embedding], 
                n_results=top_k,
                where=where_filter
            )
            logger.info(f"ğŸ” ChromaDB ì›ì‹œ ê²°ê³¼: documents={len(results.get('documents', [[]])[0])}, ids={results.get('ids', [[]])}")
            
            return [{
                'text': doc,
                'distance': dist,
                'metadata': meta,
                'id': id
            } for doc, dist, meta, id in zip(results['documents'][0], results['distances'][0], results['metadatas'][0], results['ids'][0])]
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.db.close()

knowledge_manager = KnowledgeManager()