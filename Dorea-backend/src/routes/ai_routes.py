"""
==========================================
AI/GPT Processing Routes Module
==========================================

AI ì²˜ë¦¬ ê´€ë ¨ ëª¨ë“  ë¼ìš°íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ê¸°ëŠ¥:
- GPT ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- Vision API ì²˜ë¦¬ (ì´ë¯¸ì§€ ë¶„ì„)
- ë©€í‹° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
- AI ì§ˆë¬¸ ì‘ë‹µ

Author: Dorea Team  
Last Updated: 2024-08-22
"""

# FastAPI ê´€ë ¨ imports
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional
import json
import httpx
import asyncio
import os

# ë‚´ë¶€ ëª¨ë“ˆ imports  
from database import get_db, User, UserSettings, hash_api_key
from auth import get_current_user, create_openai_client

# Pydantic ëª¨ë¸ imports
from pydantic import BaseModel

# ==========================================
# Pydantic ëª¨ë¸ ì •ì˜ 
# ==========================================

class TextRequest(BaseModel):
    text: str

class QueryRequest(BaseModel):
    text: str
    query: str

class VisionRequest(BaseModel):
    image: str
    query: str

class MultiSegmentRequest(BaseModel):
    segments: List[Dict[str, Any]]
    query: str
    conversation_history: List[Dict[str, str]] = []

# ==========================================
# í™˜ê²½ ì„¤ì •
# ==========================================

# OLLAMA API URL
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://ollama:11434")

# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================

# 1x1 í”½ì…€ íˆ¬ëª… PNG ì´ë¯¸ì§€ (base64)
TINY_IMAGE_BASE64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII="

# ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—¬ë¶€ ìºì‹œ
multimodal_support_cache = {}

async def get_user_ai_provider_by_user(user: User, db: Session) -> tuple:
    """JWT ì‚¬ìš©ìì˜ AI ëª¨ë¸ ì„¤ì • ì¡°íšŒ - user_id ê¸°ë°˜ìœ¼ë¡œ ì¡°íšŒ"""
    # ğŸ”¥ ì‚¬ìš©ì IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • ì¡°íšŒ (API í‚¤ì™€ ë…ë¦½ì )
    settings = db.query(UserSettings).filter(
        UserSettings.user_id == user.id
    ).first()
    
    if not settings:
        # ê¸°ë³¸ê°’ ë°˜í™˜ (GPT)
        return "gpt", None
    
    return settings.selected_model_provider, settings.selected_ollama_model

async def call_ollama_api(model_name: str, messages: list, stream: bool = False, images: list = None) -> dict:
    """Ollama API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
    try:
        # Ollama API ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        ollama_messages = []
        for msg in messages:
            if msg["role"] == "system":
                ollama_messages.append({"role": "system", "content": msg["content"]})
            elif msg["role"] == "user":
                user_message = {"role": "user", "content": msg["content"]}
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
                if images:
                    user_message["images"] = images
                ollama_messages.append(user_message)
        
        payload = {
            "model": model_name,
            "messages": ollama_messages,
            "stream": stream,
            "options": {
                "keep_alive": "60s"  # ëª¨ë¸ì„ 60ì´ˆ ë™ì•ˆ ë©”ëª¨ë¦¬ì— ìœ ì§€ í›„ ìë™ í•´ì œ
            }
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{OLLAMA_API_URL}/api/chat",
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                if stream:
                    return response  # ìŠ¤íŠ¸ë¦¬ë°ì˜ ê²½ìš° ì‘ë‹µ ê°ì²´ ìì²´ë¥¼ ë°˜í™˜
                else:
                    data = response.json()
                    return {"result": data.get("message", {}).get("content", "")}
            else:
                # APIì—ì„œ ë°›ì€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ì—¬ ì˜ˆì™¸ ë°œìƒ
                error_details = response.text
                raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code} - {error_details}")
                
    except Exception as e:
        raise Exception(f"Ollama ì—°ê²° ì˜¤ë¥˜: {str(e)}")

async def check_ollama_model_multimodal_support(model_name: str) -> bool:
    """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ìš”ì²­ìœ¼ë¡œ Ollama ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—¬ë¶€ í™•ì¸"""
    # ìºì‹œì—ì„œ í™•ì¸
    if model_name in multimodal_support_cache:
        return multimodal_support_cache[model_name]
    
    try:
        # ì‘ì€ ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        payload = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "What is in this image?", "images": [TINY_IMAGE_BASE64]}
            ],
            "stream": False
        }
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(f"{OLLAMA_API_URL}/api/chat", json=payload)
            
            if response.status_code == 200:
                # ì„±ê³µí•˜ë©´ ë©€í‹°ëª¨ë‹¬ ì§€ì›
                multimodal_support_cache[model_name] = True
                return True
            else:
                # ì—ëŸ¬ ì‘ë‹µ ë‚´ìš© ìì„¸íˆ í™•ì¸
                try:
                    error_data = response.json()
                    error_msg = error_data.get("error", "Unknown error")
                    print(f"ğŸ” ë©€í‹°ëª¨ë‹¬ í…ŒìŠ¤íŠ¸ ({model_name}): {response.status_code} - {error_msg}")
                    print(f"ğŸ” ì „ì²´ ì˜¤ë¥˜ ì‘ë‹µ: {error_data}")
                    
                    # ë‹¤ì–‘í•œ ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ í™•ì¸
                    error_lower = error_msg.lower()
                    if any(keyword in error_lower for keyword in ["image", "vision", "multimodal", "support"]):
                        multimodal_support_cache[model_name] = False
                        return False
                except Exception as parse_error:
                    print(f"ğŸ” ë©€í‹°ëª¨ë‹¬ í…ŒìŠ¤íŠ¸ ({model_name}): {response.status_code} - ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                    print(f"ğŸ” ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text}")
        
        # ê¸°íƒ€ ì—ëŸ¬ëŠ” ë¯¸ì§€ì›ìœ¼ë¡œ ì²˜ë¦¬
        multimodal_support_cache[model_name] = False
        return False
        
    except Exception as e:
        print(f"ğŸ” ë©€í‹°ëª¨ë‹¬ ì§€ì› í…ŒìŠ¤íŠ¸ ì˜ˆì™¸ ({model_name}): {e}")
        multimodal_support_cache[model_name] = False
        return False

async def send_openai_query(query: str, api_key: str, base64_image: Optional[str] = None):
    """OpenAI API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜"""
    try:
        client = create_openai_client(api_key)
        
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            }
        ]
        
        if base64_image:
            # ğŸ†• base64 ì´ë¯¸ì§€ ì •ë¦¬ - dataURL í—¤ë” ì œê±°
            if base64_image.startswith('data:image'):
                base64_image = base64_image.split(',')[1]
            
            messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": query},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                    }
                ]
            })
            model = "gpt-4o"
        else:
            messages.append({
                "role": "user", 
                "content": query
            })
            model = "gpt-4o"
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )
        
        return {"result": response.choices[0].message.content.strip()}
        
    except Exception as e:
        pass  # ë¡œê·¸ ì œê±°
        raise HTTPException(status_code=500, detail=f"OpenAI API ì˜¤ë¥˜: {str(e)}")

# ==========================================
# ë¼ìš°í„° ì„¤ì •
# ==========================================

router = APIRouter(prefix="/api", tags=["AI"])

# ==========================================
# AI ì²˜ë¦¬ ë¼ìš°íŠ¸ 
# ==========================================

# TODO: í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ /multi-segment-streamë§Œ ì‚¬ìš© ì¤‘
@router.post("/stream")
async def stream_gpt_response(
    request: QueryRequest, 
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """AI ì‘ë‹µì„ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜ - GPT/Ollama ë¶„ê¸° ì§€ì›"""
    
    # ğŸ”¥ ì‚¬ìš©ì AI ì„¤ì •ì„ ë¯¸ë¦¬ ì¡°íšŒ  
    try:
        provider, ollama_model = await get_user_ai_provider_by_user(current_user, db)
    except:
        provider, ollama_model = "gpt", None
    
    # GPT ì‚¬ìš© ì‹œì—ë§Œ API í‚¤ í™•ì¸
    if provider == "gpt" and not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="GPT ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
    
    def generate_stream():  # ğŸ”¥ ì¼ë°˜ defë¡œ ë³€ê²½í•˜ë˜ ë‚´ë¶€ì—ì„œ async ì²˜ë¦¬
        try:
            # ğŸ”¥ asyncio loopë¥¼ í•­ìƒ ë¯¸ë¦¬ ìƒì„±
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            if request.text:
                query = f"""ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì¤˜:

í…ìŠ¤íŠ¸:
{request.text}

ì§ˆë¬¸:
{request.query}
"""
            else:
                query = request.query
            
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            # ğŸ”¥ ì¦‰ì‹œ ì‹œì‘ ì‹ í˜¸
            yield f"data: {json.dumps({'type': 'start', 'provider': provider})}\n\n"
            
            if provider == "ollama" and ollama_model:
                # Ollama API í˜¸ì¶œ - ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                try:
                    ollama_response = loop.run_until_complete(call_ollama_api(ollama_model, messages, stream=True))
                    
                    # Ollama ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ - ë™ê¸° ë°©ì‹
                    for line_bytes in ollama_response.iter_lines():
                        if line_bytes:
                            try:
                                line = line_bytes.decode('utf-8') if isinstance(line_bytes, bytes) else line_bytes
                                data = json.loads(line)
                                if "message" in data and "content" in data["message"]:
                                    content = data["message"]["content"]
                                    if content:
                                        yield f"data: {json.dumps({'type': 'chunk', 'content': content})}\n\n"
                                
                                if data.get("done", False):
                                    break
                            except json.JSONDecodeError:
                                continue
                                
                except Exception as e:
                    yield f"data: {json.dumps({'type': 'error', 'error': f'Ollama ì˜¤ë¥˜: {str(e)}'})}\n\n"
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"  # ì—ëŸ¬ ì‹œì—ë„ done ì‹ í˜¸ ì „ì†¡
                    return
            else:
                # GPT API í˜¸ì¶œ (ê¸°ë³¸ê°’)
                client = create_openai_client(current_user.api_key)
                
                stream = client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=1000,
                    temperature=0.7,
                    stream=True
                )
                
                # ê° ì²­í¬ë¥¼ ë°›ëŠ” ì¦‰ì‹œ yield
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        yield f"data: {json.dumps({'type': 'chunk', 'content': content})}\n\n"
            
            # ì™„ë£Œ ì‹ í˜¸
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "X-Accel-Buffering": "no"
        }
    )

# TODO: í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ /multi-segment-streamë§Œ ì‚¬ìš© ì¤‘
@router.post("/vision-stream")
async def stream_vision_response(
    request: VisionRequest,
    current_user: User = Depends(get_current_user)
):
    """Vision API ì‘ë‹µì„ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜"""
    
    # API í‚¤ í™•ì¸
    if not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
    
    def generate_stream():  # ğŸ”¥ async def ëŒ€ì‹  def ì‚¬ìš©!
        try:
            client = create_openai_client(current_user.api_key)
            
            base64_image = request.image
            if "base64," in base64_image:
                base64_image = base64_image.split("base64,")[1]
            
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": request.query},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                        }
                    ]
                }
            ]
            
            yield f"data: {json.dumps({'type': 'start'})}\n\n"
            
            stream = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    yield f"data: {json.dumps({'type': 'chunk', 'content': content})}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "X-Accel-Buffering": "no"
        }
    )

# === ë©€í‹° ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ===

# TODO: í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ /multi-segment-streamë§Œ ì‚¬ìš© ì¤‘
@router.post("/multi-segment")
async def analyze_multi_segments(
    request: MultiSegmentRequest,
    current_user: User = Depends(get_current_user)
):
    """ë‹¤ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„"""
    # API í‚¤ í™•ì¸
    if not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
        
    try:
        client = create_openai_client(current_user.api_key)
        
        # ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ë¶„ì„í•´ì„œ ë©”ì‹œì§€ êµ¬ì„±
        content_parts = []
        has_images = False
        
        # í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤ ë¨¼ì € ì²˜ë¦¬
        text_context = f"ì‚¬ìš©ì ì§ˆë¬¸: {request.query}\n\n"
        text_context += f"ë‹¤ìŒ {len(request.segments)}ê°œ ì˜ì—­ì„ ì¢…í•©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n"
        
        for i, segment in enumerate(request.segments):
            if segment['type'] == 'text':
                text_context += f"[ì˜ì—­ {i+1}] í˜ì´ì§€ {segment.get('page', '?')}:\n"
                text_context += f"{segment['content']}\n\n"
            elif segment['type'] == 'image':
                has_images = True
                text_context += f"[ì˜ì—­ {i+1}] í˜ì´ì§€ {segment.get('page', '?')}: {segment.get('description', 'ì´ë¯¸ì§€')}\n\n"
        
        if has_images:
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ Vision API ì‚¬ìš©
            content_parts.append({"type": "text", "text": text_context})
            
            # ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            for segment in request.segments:
                if segment['type'] == 'image' and segment.get('content'):
                    image_data = segment['content']
                    if "base64," in image_data:
                        image_data = image_data.split("base64,")[1]
                    
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{image_data}"}
                    })
            
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¢…í•©í•˜ì—¬ í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": content_parts
                }
            ]
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=1500,
                temperature=0.7
            )
        else:
            # í…ìŠ¤íŠ¸ë§Œ ìˆìœ¼ë©´ ì¼ë°˜ GPT ì‚¬ìš©
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": text_context
                }
            ]
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=1500,
                temperature=0.7
            )
        
        return {"result": response.choices[0].message.content.strip()}
        
    except Exception as e:
        pass  # ë¡œê·¸ ì œê±°
        raise HTTPException(status_code=500, detail=f"ë©€í‹° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")

@router.post("/multi-segment-stream")
async def stream_multi_segment_response(
    request: MultiSegmentRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ë‹¤ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ì„ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜ - GPT/Ollama ë¶„ê¸° ì§€ì›"""
    
    # ğŸ”¥ ì‚¬ìš©ì AI ì„¤ì •ì„ ë¯¸ë¦¬ ì¡°íšŒ  
    try:
        provider, ollama_model = await get_user_ai_provider_by_user(current_user, db)
    except:
        provider, ollama_model = "gpt", None

    # GPT ì‚¬ìš© ì‹œì—ë§Œ API í‚¤ í™•ì¸
    if provider == "gpt" and not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="GPT ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
    
    
    def generate_stream():  # ğŸ”¥ ì¼ë°˜ defë¡œ ë³€ê²½í•˜ë˜ ë‚´ë¶€ì—ì„œ async ì²˜ë¦¬
        try:
            # ğŸ”¥ asyncio loopë¥¼ í•­ìƒ ë¯¸ë¦¬ ìƒì„±
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            content_parts = []
            has_images = False
            image_data_list = []  # Ollamaìš© ì´ë¯¸ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
            text_context = f"## í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ (ìµœìš°ì„ ):\n{request.query}\n\n"
            text_context += f"## ì°¸ê³ í•  ë¬¸ì„œ ì˜ì—­ ({len(request.segments)}ê°œ):\n"
            
            for i, segment in enumerate(request.segments):
                if segment['type'] == 'text':
                    text_context += f"[ì˜ì—­ {i+1}] í˜ì´ì§€ {segment.get('page', '?')}:\n"
                    text_context += f"{segment['content']}\n\n"
                elif segment['type'] == 'image':
                    has_images = True
                    text_context += f"[ì˜ì—­ {i+1}] í˜ì´ì§€ {segment.get('page', '?')}: {segment.get('description', 'ì´ë¯¸ì§€')}\n\n"
                    # Ollamaìš© ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (base64) - GPTì™€ ë™ì¼í•˜ê²Œ 'content' í•„ë“œ ì‚¬ìš©
                    if 'content' in segment and segment['content']:
                        image_data = segment['content']
                        # ë°ì´í„° URL í˜•ì‹(e.g., "data:image/png;base64,iVBOR...")ì¸ ê²½ìš° ìˆœìˆ˜ base64 ë°ì´í„°ë§Œ ì¶”ì¶œ
                        if "base64," in image_data:
                            image_data = image_data.split("base64,")[1]
                        image_data_list.append(image_data)
            
            # ğŸ”¥ ì¦‰ì‹œ ì‹œì‘ ì‹ í˜¸ (ì–´ë–¤ ì œê³µìì¸ì§€ ì•Œë ¤ì¤Œ)
            yield f"data: {json.dumps({'type': 'start', 'provider': provider})}\n\n"
            
            # ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œ Ollama ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—¬ë¶€ í™•ì¸
            if has_images and provider == "ollama":
                # ìºì‹œì— ì—†ëŠ” ê²½ìš°ì—ë§Œ í…ŒìŠ¤íŠ¸ ì¤‘ ë©”ì‹œì§€ í‘œì‹œ
                if ollama_model not in multimodal_support_cache:
                    yield f"data: {json.dumps({'type': 'info', 'message': f'ëª¨ë¸ {ollama_model}ì˜ ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...'})}\n\n"
                
                multimodal_support = loop.run_until_complete(check_ollama_model_multimodal_support(ollama_model))
                if not multimodal_support:
                    error_msg = f'ì„ íƒëœ ëª¨ë¸ {ollama_model}ì€ ì´ë¯¸ì§€/í‘œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.'
                    yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"  # í´ë¼ì´ì–¸íŠ¸ ëŒ€ê¸° ë°©ì§€ë¥¼ ìœ„í•´ done ì‹ í˜¸ ì „ì†¡
                    return
                else:
                    yield f"data: {json.dumps({'type': 'info', 'message': f'âœ… ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ {ollama_model}ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.'})}\n\n"
            
            if provider == "ollama" and ollama_model:
                # Ollama API í˜¸ì¶œ (í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì§€ì›)
                try:
                    # ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì§ˆë¬¸)
                    messages = [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ê³¼ê±° ëŒ€í™”ëŠ” ì°¸ê³ ë§Œ í•˜ê³ , í˜„ì¬ ìš”ì²­ëœ ì‘ì—…(ìš”ì•½, ë²ˆì—­, ë¶„ì„ ë“±)ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”."
                        }
                    ]
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
                    if request.conversation_history:
                        for msg in request.conversation_history:
                            messages.append({
                                "role": msg.get("role", "user"),
                                "content": msg.get("content", "")
                            })
                    
                    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
                    messages.append({
                        "role": "user",
                        "content": text_context
                    })
                    
                    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì „ë‹¬, ì—†ìœ¼ë©´ None
                    images_to_send = image_data_list if has_images else None
                    print(f"ğŸ” ì´ë¯¸ì§€ ì „ì†¡ ë””ë²„ê·¸: has_images={has_images}, ì´ë¯¸ì§€ ê°œìˆ˜={len(image_data_list) if image_data_list else 0}")
                    if images_to_send and len(images_to_send) > 0:
                        print(f"ğŸ” ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ë°ì´í„° ê¸¸ì´: {len(images_to_send[0])}")
                    
                    ollama_response = loop.run_until_complete(call_ollama_api(ollama_model, messages, stream=True, images=images_to_send))
                    
                    # Ollama ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ - ë™ê¸° ë°©ì‹
                    for line_bytes in ollama_response.iter_lines():
                        if line_bytes:
                            try:
                                line = line_bytes.decode('utf-8') if isinstance(line_bytes, bytes) else line_bytes
                                data = json.loads(line)
                                if "message" in data and "content" in data["message"]:
                                    content = data["message"]["content"]
                                    if content:
                                        yield f"data: {json.dumps({'type': 'chunk', 'content': content})}\n\n"
                                
                                if data.get("done", False):
                                    break
                            except json.JSONDecodeError:
                                continue
                                
                except Exception as e:
                    yield f"data: {json.dumps({'type': 'error', 'error': f'Ollama ì˜¤ë¥˜: {str(e)}'})}\n\n"
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"  # ì—ëŸ¬ ì‹œì—ë„ done ì‹ í˜¸ ì „ì†¡
                    return
            else:
                # GPT API í˜¸ì¶œ (ê¸°ë³¸ê°’ ë˜ëŠ” ì´ë¯¸ì§€ í¬í•¨)
                client = create_openai_client(current_user.api_key)
                
                if has_images:
                    content_parts.append({"type": "text", "text": text_context})
                    
                    for segment in request.segments:
                        if segment['type'] == 'image' and segment.get('content'):
                            image_data = segment['content']
                            if "base64," in image_data:
                                image_data = image_data.split("base64,")[1]
                            
                            content_parts.append({
                                "type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{image_data}"}
                            })
                    
                    # ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì§ˆë¬¸)
                    messages = [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¢…í•©í•˜ì—¬ í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        }
                    ]
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
                    if request.conversation_history:
                        for msg in request.conversation_history:
                            messages.append({
                                "role": msg.get("role", "user"),
                                "content": msg.get("content", "")
                            })
                    
                    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
                    messages.append({
                        "role": "user",
                        "content": content_parts
                    })
                else:
                    # ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì§ˆë¬¸)
                    messages = [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì—¬ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ê³¼ê±° ëŒ€í™”ëŠ” ì°¸ê³ ë§Œ í•˜ê³ , í˜„ì¬ ìš”ì²­ëœ ì‘ì—…(ìš”ì•½, ë²ˆì—­, ë¶„ì„ ë“±)ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”."
                        }
                    ]
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
                    if request.conversation_history:
                        for msg in request.conversation_history:
                            messages.append({
                                "role": msg.get("role", "user"),
                                "content": msg.get("content", "")
                            })
                    
                    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
                    messages.append({
                        "role": "user",
                        "content": text_context
                    })
                
                
                stream = client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.7,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        yield f"data: {json.dumps({'type': 'chunk', 'content': content})}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "X-Accel-Buffering": "no"
        }
    )
# ê¸°ì¡´ send_openai_query í•¨ìˆ˜ë¥¼ ì´ê²ƒìœ¼ë¡œ êµì²´
async def send_openai_query(query: str, api_key: str, base64_image: Optional[str] = None):
    try:
        client = create_openai_client(current_user.api_key)
        
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            }
        ]
        
        if base64_image:
            # ğŸ†• base64 ì´ë¯¸ì§€ ì •ë¦¬ - dataURL í—¤ë” ì œê±°
            if base64_image.startswith('data:image'):
                base64_image = base64_image.split(',')[1]
            
            messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": query},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                    }
                ]
            })
            model = "gpt-4o"
        else:
            messages.append({
                "role": "user", 
                "content": query
            })
            model = "gpt-4o"
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )
        
        return {"result": response.choices[0].message.content.strip()}
        
    except Exception as e:
        pass  # ë¡œê·¸ ì œê±°
        raise HTTPException(status_code=500, detail=f"OpenAI API ì˜¤ë¥˜: {str(e)}")

# AI ì§ˆë¬¸ ì‘ë‹µ (GPT/Ollama ë¶„ê¸° ì§€ì›)
# TODO: í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ /multi-segment-streamë§Œ ì‚¬ìš© ì¤‘
@router.post("/ask")
async def ask_gpt(
    request: QueryRequest, 
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """AIì—ê²Œ ì§ˆë¬¸í•˜ê¸° - GPT/Ollama ë¶„ê¸° ì§€ì›"""
    
    # API í‚¤ í™•ì¸
    if not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
    
    try:
        # ì‚¬ìš©ì AI ì„¤ì • ì¡°íšŒ
        provider, ollama_model = await get_user_ai_provider_by_user(current_user, db)
        
        if request.text:
            query = f"""ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì¤˜:

í…ìŠ¤íŠ¸:
{request.text}

ì§ˆë¬¸:
{request.query}
"""
        else:
            query = request.query
        
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            },
            {
                "role": "user",
                "content": query
            }
        ]
        
        if provider == "ollama" and ollama_model:
            # Ollama API í˜¸ì¶œ
            result = await call_ollama_api(ollama_model, messages, stream=False)
            return result
        else:
            # GPT API í˜¸ì¶œ (ê¸°ë³¸ê°’)
            return await send_openai_query(query, api_key)
            
    except Exception as e:
        pass  # ë¡œê·¸ ì œê±°
        raise HTTPException(status_code=500, detail=f"AI ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")


# TODO: í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ /multi-segment-streamë§Œ ì‚¬ìš© ì¤‘
@router.post("/vision")
async def vision_analysis(request: VisionRequest, current_user: User = Depends(get_current_user)):
    """ì´ë¯¸ì§€ë¥¼ GPT Visionìœ¼ë¡œ ë¶„ì„"""
    
    # API í‚¤ í™•ì¸
    if not current_user.api_key:
        raise HTTPException(
            status_code=400, 
            detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
        )
    
    try:
        print(f"ğŸ” Vision ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“ Query: {request.query}")
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë°ì´í„° ê¸¸ì´: {len(request.image) if request.image else 0}")
        
        # ğŸ†• ì´ë¯¸ì§€ í¬ê¸° ì²´í¬
        if len(request.image) > 100000:  # 100KB ì œí•œ
            print(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¼: {len(request.image)} bytes")
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ ì˜ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        # dataURL í˜•ì‹ì—ì„œ base64 ì¶”ì¶œ
        base64_image = request.image
        if "base64," in base64_image:
            base64_image = base64_image.split("base64,")[1]
            print(f"âœ… Base64 ì¶”ì¶œ ì™„ë£Œ, ê¸¸ì´: {len(base64_image)}")
        
        result = await send_openai_query(request.query, api_key, base64_image)
        print(f"âœ… OpenAI ì‘ë‹µ ë°›ìŒ")
        
        return result
        
    except Exception as e:
        print(f"âŒ Vision API ì—ëŸ¬: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Vision API ì˜¤ë¥˜: {str(e)}")
    


# TODO: backend.pyì—ì„œ ë‹¤ìŒ í•¨ìˆ˜ë“¤ì„ ë³µì‚¬í•´ì„œ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°:
# 1. @router.post("/stream") - stream_gpt_response
# 2. @router.post("/vision-stream") - stream_vision_response
# 3. @router.post("/multi-segment") - analyze_multi_segments  
# 4. @router.post("/multi-segment-stream") - stream_multi_segment_response
# 5. @router.post("/ask") - ask_gpt
# 6. @router.post("/vision") - vision_analysis

# ì´ ë¼ìš°íŠ¸ë“¤ì„ ë³µì‚¬í•  ë•Œ:
# - @router.post â†’ @router.postë¡œ ë³€ê²½
# - "/"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ë¡œì—ì„œ "/" ì œê±° (ì˜ˆ: "/stream" â†’ "/stream")